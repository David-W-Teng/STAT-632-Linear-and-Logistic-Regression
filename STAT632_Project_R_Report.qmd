---
title: "STAT632 Project (Advertisement Sales Dataset)"
author: "David Teng"
date: "`r Sys.Date()`"
output: pdf_document
format: pdf
editor: visual
---

## **About Dataset**

### **Advertisement Sales Dataset**

The Advertisement Sales dataset is a collection of data points used to analyze the impact of advertising on sales. This dataset consists of 200 entries, each representing a unique observation with data on various types of media advertising and corresponding sales figures.

### Load Libraries and Data

```{r}
# Load necessary libraries
library(MASS)          # for boxcox
library(glmnet)        # for LASSO
library(randomForest)  # for Random Forest
library(car)           # for VIF
library(ggplot2)       # for nice plots
library(caret)         # for model validation
library(dplyr)         # for data manipulation

# Load data
adver <- read.csv("Advertising And Sales.csv")

# Quick overview
summary(adver)
str(adver)

# Pairwise scatterplot with title
pairs(Sales ~ TV + Radio + Newspaper, data = adver,
      main = "Pairwise Scatterplot of Sales and
      Advertising Channels")


```

### Data Preparation

```{r}
# Check for missing values
colSums(is.na(adver))
```

### Base Multiple Linear Regression

```{r}
# Base Multiple Linear Regression
lm_full <- lm(Sales ~ TV + Radio + Newspaper, data = adver)
summary(lm_full)
```

### Interpretation of Base Multiple Linear Regression:

We fitted a multiple linear regression model to predict **Sales** based on **TV**, **Radio**, and **Newspaper** advertising budgets.

-   The **Intercept** is estimated at **3.00** (p \< 0.001), meaning that when advertising budgets are zero, the expected sales would be about 3,000 units.

-   The **TV advertising budget** has a **positive and significant** effect on Sales.

    Each additional thousand dollars spent on TV is associated with an **increase of about 45.7 units** in Sales, **holding other factors constant** (p \< 0.001).

-   The **Radio advertising budget** also has a **positive and significant** effect on Sales.

    Each additional thousand dollars spent on Radio is associated with an **increase of about 187.1 units** in Sales (p \< 0.001).

-   The **Newspaper advertising budget** is **not statistically significant** (p = 0.822), suggesting that spending on Newspaper ads does **not have a meaningful effect** on Sales in this model.

### Goodness of Fit:

-   The **Multiple R-squared** is **0.8958**, meaning the model explains about **89.6%** of the variance in Sales.

-   The **Adjusted R-squared** is **0.8942**, which adjusts for the number of predictors and confirms the model still fits the data very well.

-   The overall **F-statistic** is highly significant (p \< 2.2e-16), indicating that the model provides a better fit than a model with no predictors.

### Conclusion:

The model shows that TV and Radio advertising significantly increase sales, while Newspaper advertising does not. The model explains 89.6% of the variance in sales, and overall, it fits the data very well (F-test p \< 0.001).

### Reduced Model (remove Newspaper)

```{r}
# Reduced Model (remove Newspaper)
lm_reduced <- lm(Sales ~ TV + Radio, data = adver)
summary(lm_reduced)
```

### Interpretation of Reduced Model (TV + Radio only):

We fitted a reduced multiple linear regression model to predict **Sales** using only **TV** and **Radio** advertising budgets (after removing Newspaper).

-   The **Intercept** is estimated at **2.98** (p \< 0.001), meaning that when TV and Radio advertising expenditures are zero, the expected sales would be about **2,980 units**.

-   The **TV advertising budget** remains a **positive and highly significant predictor** of Sales.

    Each additional thousand dollars spent on TV advertising is associated with an **increase of approximately 45.7 units** in Sales, **holding Radio constant** (p \< 0.001).

-   The **Radio advertising budget** also remains **positive and highly significant**.

    Each additional thousand dollars spent on Radio advertising is associated with an **increase of approximately 186.4 units** in Sales (p \< 0.001).

### Goodness of Fit:

-   The **Multiple R-squared** is **0.8957**, indicating that about **89.6%** of the variance in Sales is explained by TV and Radio budgets.

-   The **Adjusted R-squared** is **0.8947**, very close to the full model, suggesting that removing Newspaper **did not harm model fit**.

-   The model's **F-statistic** is **highly significant** (p \< 2.2e-16), showing the model overall is statistically significant.

### Model Comparison:

-   Compared to the full model (TV + Radio + Newspaper), the reduced model achieves **almost identical R-squared** with fewer predictors.

-   Based on the **partial F-test** and **adjusted R-squared**, we conclude that **Newspaper** advertising is **not necessary** for predicting Sales.

### Conclusion:

The reduced model including only TV and Radio advertising performs just as well as the full model. Both TV and Radio advertising expenditures have significant positive effects on Sales, while Newspaper advertising was found to be unnecessary. The reduced model explains about 89.6% of the variance in Sales and provides a simpler, equally effective prediction model.

### Compare Models: Full vs Reduced:

Hypotheses for Model Comparison:

$H_0: \beta_{Newspaper} = 0$ (The coefficient for **Newspaper** is equal to zero which means Newspaper does **not** improve the model.)\

vs. $H_1: \beta_{Newspaper} \neq 0$

(The coefficient for **Newspaper** is **not** equal to zero which means Newspaper **does** improve the model.)

```{r}
# Compare Models: Full vs Reduced
anova(lm_reduced, lm_full) # partial F-test 
```

### Interpretation of Model Comparison (Full vs Reduced):

We conducted a **partial F-test** to formally compare the full model (**Sales \~ TV + Radio + Newspaper**) with the reduced model (**Sales \~ TV + Radio**).

From the ANOVA table:

-   The test statistic is **F = 0.0507**, with a corresponding **p-value = 0.8221**.

-   The p-value is **much greater than 0.05**, meaning we **fail to reject** the null hypothesis.

**Interpretation**:

-   There is **no significant evidence** that adding **Newspaper** as a predictor improves the model.

-   Therefore, the **simpler model with only TV and Radio** is preferred.

### Additional Note:

-   The Residual Sum of Squares (RSS) only **slightly decreased** from **563.09** to **562.95** after adding Newspaper, which is not meaningful.

-   This further confirms that **Newspaper** is not a useful predictor for Sales.

### Conclusion:

Since the partial F-test (p = 0.8221), we **fail to reject** $H_0$​.\

This means **Newspaper does not significantly improve** the model. Thus, the reduced model with only TV and Radio is sufficient.

### Adjusted R-squared comparison

```{r}
# Adjusted R-squared comparison
summary(lm_full)$adj.r.squared # Full model (TV + Radio + Newspaper
summary(lm_reduced)$adj.r.squared # Reduced model (TV + Radio only)
```

### Adjusted R-squared Comparison Interpretation:

The **adjusted R-squared** for the reduced model (**0.8947**) is **slightly higher** than that of the full model (**0.8942**).

-   **Adjusted R-squared** adjusts for the number of predictors in the model.

-   A **higher adjusted R-squared** suggests that the reduced model **fits the data better**, even though it uses **fewer predictors**.

-   Therefore, the model including only **TV** and **Radio** provides a **better and simpler fit** than the model that also includes **Newspaper**.

### Conclusion:

The reduced model (TV + Radio) has a slightly higher adjusted R-squared than the full model, indicating a better fit with fewer predictors.

### Result: Slightly better adjusted R² for reduced model → remove Newspaper

### Pairwise scatterplot

```{r}
pairs(Sales ~ TV + Radio, data = adver)
```

### Interpretation of Pairwise Scatterplot (Sales, TV, Radio):

The pairwise scatterplot shows the relationships between **Sales**, **TV**, and **Radio** advertising:

-   **Sales vs TV**: There is a **strong positive linear relationship**. As spending on TV advertising increases, Sales also tend to increase. The pattern is clear and linear, supporting the use of TV as a predictor in a linear regression model.

-   **Sales vs Radio**: A **moderate positive linear relationship** is also observed. Though more spread out than the TV relationship, the trend is still upward, suggesting Radio advertising has a meaningful impact on Sales.

-   **TV vs Radio**: The scatterplot shows **no strong correlation** between TV and Radio advertising budgets. The points are scattered without a clear pattern, suggesting that TV and Radio are **not highly collinear**, which is good for regression modeling.

### Conclusion:

Sales shows strong positive correlation with TV advertising and moderate positive correlation with Radio advertising. TV and Radio budgets appear to be largely independent.

### Diagnostic Plots for lm_reduced

```{r}
# Diagnostic Plots for lm_reduced
par(mfrow=c(2,2))
plot(lm_reduced)
```

### Diagnostic Plots Interpretation (for Reduced Model):

These diagnostic plots help assess the assumptions of the multiple linear regression model:

### 1. **Residuals vs Fitted**

-   This plot checks for **linearity** and **homoscedasticity**.

-   The residuals appear to be randomly scattered around the horizontal line, indicating that:

    The relationship between predictors and response is likely **linear**.

    There is **no clear pattern**, suggesting **constant variance** (no heteroscedasticity).

### 2. **Normal Q-Q Plot**

-   This plot checks for **normality of residuals**.

-   The residual points mostly fall along the straight line, indicating that the residuals are **approximately normally distributed**.

### 3. **Scale-Location Plot**

-   This plot also checks for **homoscedasticity**, using standardized residuals.

-   The red line is mostly flat and the spread of residuals is consistent across fitted values, suggesting **homogeneity of variance**.

### 4. **Residuals vs Leverage**

-   This plot identifies **influential points** that may disproportionately affect the model.

-   There are no points with **unusually high leverage** or **extreme residuals**, indicating that there are **no strong outliers or influential observations**.

### Conclusion:

The diagnostic plots suggest that the reduced model meets the assumptions of linearity, normality, constant variance, and no influential outliers. Thus, the model appears appropriate for inference and prediction.

### Residuals vs Fitted for lm2

```{r}
# Residuals vs Fitted for lm_reduced
plot(fitted(lm_reduced), resid(lm_reduced),
     xlab="Fitted values", ylab="Residuals",
     main="Residuals vs Fitted")
abline(h=0, col="red")
```

### Interpretation:

The Residuals vs Fitted plot shows that residuals are randomly scattered around zero with no strong pattern, supporting the assumptions of linearity and constant variance.

### QQ plot for residuals (Normality)

```{r}
# QQ plot for residuals (Normality)
qqnorm(rstandard(lm_reduced))
qqline(rstandard(lm_reduced))
```

### Interpretation:

The Q-Q plot suggests that the residuals are **approximately normally distributed**, with **minor deviations at the tails**. This does **not seriously violate** the normality assumption required for multiple linear regression.

### Boxplots to check for outliers in predictors

```{r}
# Boxplots to check for outliers in predictors
boxplot(adver[, c("TV", "Radio", "Newspaper")], 
        main="Boxplots of Advertising Budgets", 
        col=c("green", "red", "orange"))

```

### Interpretation:

The boxplots show that **TV has the largest budget range**, while **Newspaper advertising contains a few outliers**. No extreme values are observed for TV or Radio. This insight helps explain why Newspaper may not be a strong predictor in the regression model — its distribution is more scattered and includes outlying values.

### Why Use a Box-Cox Transformation for `lm_full?`

### For Model (`lm_full: Sales ~ TV + Radio + Newspaper`)

-   We already checked the **residuals vs fitted plot** and **QQ plot**, which were **mostly okay**, but:

    There was **some non-linearity** and **slight skewness** in the residuals.

-   So using `boxcox()` helps :

    **Confirm whether transformation is needed**, and

    **Find the best power transformation** (e.g., log, sqrt, etc.) to improve the model.

```{r}
# Box-Cox transformation for lm_full
boxcox(lm_full, lambda = seq(-2, 2, 0.1),
       main = "Box-Cox Transformation for Sales")
```

### Interpretation:

The Box-Cox transformation plot indicates that the optimal $\lambda$ is close to 1, and **no transformation of the response variable is necessary**. This supports using the original Sales variable in the multiple linear regression model.

### Check Influential Observations

```{r}
# Check Influential Observations
influence.measures(lm_reduced)
plot(lm_reduced, which=4) # Cook's distance
plot(lm_reduced, which=5) # Residuals vs Leverage
```

### Influential Observations and Outlier Diagnostics:

We assessed potential influential data points using **Cook’s Distance** and the **Residuals vs Leverage** plot.

### Cook’s Distance Plot:

-   **Cook’s Distance** measures how much a single observation influences the fitted regression coefficients.

-   Points **6**, **36**, and especially **131** stand out with the **highest Cook’s distances**.

-   However, none of the Cook’s distances exceed the common rule-of-thumb threshold of **1**, indicating **no extremely influential outliers**.

### Residuals vs Leverage Plot:

-   This plot highlights observations with both **high leverage** and **large residuals**, which can be particularly influential.

-   Observations **6**, **36**, and **131** are again labeled and lie **furthest from the center**.

-   Observation **131** shows **moderately high leverage and a notable residual**, suggesting it has **some influence**, but **not enough to distort the model**.

### Conclusion:

While observations 6, 36, and 131 show some degree of influence, none exceed critical thresholds for Cook’s distance or leverage. Therefore, we conclude that there are **no influential outliers** that threaten the validity of the model.

### Check multicollinearity

```{r}
# Check multicollinearity
vif(lm_reduced)  # Variance Inflation Factors
```

Common rule of thumb:

-   **VIF \> 5** may indicate moderate multicollinearity.

-   **VIF \> 10** indicates serious multicollinearity problems.

### Conclusion:

VIF values **close to 1** indicate **no multicollinearity**.

# --- Model Extensions ---

### 1. Three-Way Interaction Model

```{r}
# Three-way interaction model
lm_interaction <- lm(Sales ~ TV * Radio * Newspaper, data = adver)
summary(lm_interaction)

```

### 2. Three-Way Quadratic Model

```{r}
# Three-way Quadratic Model using lm()
lm_quadratic <- lm(
  Sales ~ (TV + Radio + Newspaper)^3 + 
          I(TV^2) + I(Radio^2) + I(Newspaper^2),
  data = adver
)

# View model summary
summary(lm_quadratic)

```

### Model Comparison Table

### AIC Comparison

```{r}
# List of models
models <- list(
  "Full Model"        = lm_full,
  "Reduced Model"     = lm_reduced,
  "Interaction Model" = lm_interaction,
  "Quadratic Model"   = lm_quadratic
)

# Extract statistics
model_names <- names(models)
adj_r2 <- sapply(models, function(m) summary(m)$adj.r.squared)
rse <- sapply(models, function(m) summary(m)$sigma)
aic <- sapply(models, AIC)

# Combine into a data frame
aic_table <- data.frame(
  Model = model_names,
  Adjusted_R2 = round(adj_r2, 4),
  Residual_Std_Error = round(rse, 3),
  AIC = round(aic, 2)
)

# Print the table
print(aic_table, row.names = FALSE)

```

### AIC Model Comparison Table

| Model                 | Adjusted R² | Residual Std. Error | AIC        | Notes                                                            |
|-----------------------|-------------|---------------------|------------|------------------------------------------------------------------|
| **Full Model**        | 0.8942      | 1.695               | 784.55     | Includes Newspaper; not significant                              |
| **Reduced Model**     | 0.8947      | 1.691               | 782.60     | Simpler; performs slightly better than full model                |
| **Interaction Model** | 0.9654      | 0.969               | 565.02     | Captures strong synergy; includes three-way interaction          |
| **Quadratic Model**   | **0.9845**  | **0.649**           | **407.15** | Best statistical fit; includes all squared and interaction terms |

# Recommended Final Model

Choose the **Quadratic Model** for our final project paper!

### 3. Three-Way Quadratic LASSO Model

```{r}
library(glmnet)
library(caret)

# Build full quadratic + interaction design matrix
design_formula <- ~ (TV + Radio + Newspaper)^3 + I(TV^2) + I(Radio^2) + I(Newspaper^2)

# Create model matrix (exclude intercept column)
x <- model.matrix(design_formula, data = adver)[, -1]
y <- adver$Sales

# Split into training and testing
set.seed(123)
train_idx <- createDataPartition(y, p = 0.8, list = FALSE)
x_train <- x[train_idx, ]
x_test <- x[-train_idx, ]
y_train <- y[train_idx]
y_test <- y[-train_idx]

# Fit LASSO with cross-validation
lasso_cv <- cv.glmnet(x_train, y_train, alpha = 1)
plot(lasso_cv)

# Best lambda
best_lambda <- lasso_cv$lambda.min
cat("Best lambda:", best_lambda, "\n")

# Final LASSO model
lasso_model <- glmnet(x_train, y_train, alpha = 1, lambda = best_lambda)
print(coef(lasso_model))

# Prediction and RMSE
lasso_pred <- predict(lasso_model, s = best_lambda, newx = x_test)
lasso_rmse <- sqrt(mean((y_test - lasso_pred)^2))
cat("Test RMSE:", round(lasso_rmse, 4), "\n")

```

### 4. Three-Way Random Forest

```{r}
library(randomForest)
library(caret)

# Create interaction and quadratic terms manually
adver_rf <- adver |> 
  mutate(
    TV2 = TV^2,
    Radio2 = Radio^2,
    Newspaper2 = Newspaper^2,
    TV_Radio = TV * Radio,
    TV_Newspaper = TV * Newspaper,
    Radio_Newspaper = Radio * Newspaper,
    TV_Radio_Newspaper = TV * Radio * Newspaper
  )

# Fit Random Forest with all terms
set.seed(123)
rf_model <- randomForest(
  Sales ~ TV + Radio + Newspaper +
          TV2 + Radio2 + Newspaper2 +
          TV_Radio + TV_Newspaper + Radio_Newspaper +
          TV_Radio_Newspaper,
  data = adver_rf,
  importance = TRUE
)

# Print model summary
print(rf_model)

# Variable importance
importance(rf_model)
varImpPlot(rf_model)

# RMSE on full dataset
rf_pred <- predict(rf_model, adver_rf)
rf_rmse <- sqrt(mean((adver_rf$Sales - rf_pred)^2))
cat("Random Forest RMSE:", round(rf_rmse, 4), "\n")

```

### Comparison Table

| Model                       | Test RMSE / RSE | \% Variance Explained / R² | Key Findings                                                |
|-----------------------------|-----------------|----------------------------|-------------------------------------------------------------|
| **Three-Way LASSO Model**   | 0.6339          | Not explicitly available   | Newspaper & higher-order terms shrunk to zero               |
| **Three-Way Random Forest** | 0.3093          | 98.31%                     | Strongest performance; all variables contribute nonlinearly |

### **Interpretation**:

-   **Random Forest** outperforms the LASSO model in both **RMSE** and **variance explained**, suggesting it is better at capturing complex non-linear patterns.

-   **LASSO** is valuable for **feature selection**, showing that many higher-order terms (e.g., Newspaper², Radio²) may not meaningfully contribute to prediction.

### **5. Cross validation**

```{r}
# 5. Cross-Validation of Updated Models

# Load libraries
library(caret)
library(glmnet)
library(randomForest)
library(dplyr)

# Set seed and split the data
set.seed(232)
train_idx <- createDataPartition(adver$Sales, p = 0.7, list = FALSE)
train_data <- adver[train_idx, ]
test_data <- adver[-train_idx, ]

# 1. Full Model (Three-way)
lm_full <- lm(Sales ~ TV + Radio + Newspaper, data = train_data)
full_pred <- predict(lm_full, newdata = test_data)

# 2. Reduced Model (Two predictors only)
lm_reduced <- lm(Sales ~ TV + Radio, data = train_data)
reduced_pred <- predict(lm_reduced, newdata = test_data)

# 3. Three-Way Interaction Model
lm_interaction <- lm(Sales ~ TV * Radio * Newspaper, data = train_data)
interaction_pred <- predict(lm_interaction, newdata = test_data)

# 4. Three-Way Quadratic Model
lm_poly <- lm(Sales ~ (TV + Radio + Newspaper)^3 + 
                I(TV^2) + I(Radio^2) + I(Newspaper^2), data = train_data)
poly_pred <- predict(lm_poly, newdata = test_data)

# 5. LASSO Model (Three-way)
x_train <- model.matrix(Sales ~ (TV + Radio + Newspaper)^3 + 
                        I(TV^2) + I(Radio^2) + I(Newspaper^2), data = train_data)[, -1]
x_test <- model.matrix(Sales ~ (TV + Radio + Newspaper)^3 + 
                       I(TV^2) + I(Radio^2) + I(Newspaper^2), data = test_data)[, -1]
y_train <- train_data$Sales
y_test <- test_data$Sales

lasso_cv <- cv.glmnet(x_train, y_train, alpha = 1)
lasso_model <- glmnet(x_train, y_train, alpha = 1, lambda = lasso_cv$lambda.min)
lasso_pred <- predict(lasso_model, newx = x_test)

# 6. Random Forest Model (Three-way)
train_data_rf <- train_data %>%
  mutate(
    TV2 = TV^2, Radio2 = Radio^2, Newspaper2 = Newspaper^2,
    TV_Radio = TV * Radio,
    TV_Newspaper = TV * Newspaper,
    Radio_Newspaper = Radio * Newspaper,
    TV_Radio_Newspaper = TV * Radio * Newspaper
  )

test_data_rf <- test_data %>%
  mutate(
    TV2 = TV^2, Radio2 = Radio^2, Newspaper2 = Newspaper^2,
    TV_Radio = TV * Radio,
    TV_Newspaper = TV * Newspaper,
    Radio_Newspaper = Radio * Newspaper,
    TV_Radio_Newspaper = TV * Radio * Newspaper
  )

rf_model <- randomForest(Sales ~ TV + Radio + Newspaper + TV2 + Radio2 + Newspaper2 +
                           TV_Radio + TV_Newspaper + Radio_Newspaper + TV_Radio_Newspaper,
                         data = train_data_rf)
rf_pred <- predict(rf_model, newdata = test_data_rf)

# Evaluation function
metrics <- function(pred, actual) {
  data.frame(
    RMSE = RMSE(pred, actual),
    R2 = R2(pred, actual),
    MAE = MAE(pred, actual)
  )
}

# Combine results
cv_results <- bind_rows(
  metrics(full_pred, test_data$Sales)        %>% mutate(Model = "Full"),
  metrics(reduced_pred, test_data$Sales)     %>% mutate(Model = "Reduced"),
  metrics(interaction_pred, test_data$Sales) %>% mutate(Model = "Three-Way Interaction"),
  metrics(poly_pred, test_data$Sales)        %>% mutate(Model = "Three-Way Quadratic"),
  metrics(lasso_pred, y_test)                %>% mutate(Model = "Three-Way LASSO"),
  metrics(rf_pred, test_data$Sales)          %>% mutate(Model = "Three-Way Random Forest")
)

# Sort by RMSE
cv_results <- cv_results %>% select(Model, everything()) %>% arrange(RMSE)
print(cv_results)

```

### **Cross-Validation Summary Interpretation**

-   **Three-Way LASSO** achieved the **lowest RMSE (0.554)** and **lowest MAE**, indicating it had the **best predictive accuracy** on the test set, even though R² is not directly available.

-   **Three-Way Quadratic** model also performed exceptionally well (**RMSE = 0.587**, **R² = 0.989**), capturing nearly **99% of the variance**, with strong predictive capability.

-   **Three-Way Random Forest** showed slightly higher RMSE (**0.683**) but still **explained 98.4% of the variance**, making it a solid non-parametric alternative.

-   **Three-Way Interaction** model performed well but was **less accurate** (RMSE = 0.83) than other three-way models.

-   **Reduced** and **Full** linear models showed the **poorest performance**, with **RMSEs above 1.6**, confirming that more complex structures (interactions and nonlinearity) are necessary for optimal prediction.

**Conclusion:**\

**Three-way LASSO** and **quadratic models** provide the best balance of accuracy and generalization. Simpler linear models underfit the data.

### Final Model Comparison Table

| Model                       | Adjusted R² | Original RMSE / RSE | Cross-Validated RMSE | AIC        | Included Predictors                                                               | Strength                                                   |
|-----------------------------|-------------|---------------------|----------------------|------------|-----------------------------------------------------------------------------------|------------------------------------------------------------|
| **Full Model**              | 0.8942      | 1.695               | 1.619                | 784.55     | TV, Radio, Newspaper                                                              | Newspaper not significant                                  |
| **Reduced Model**           | 0.8947      | 1.691               | 1.615                | 782.60     | TV, Radio                                                                         | Simpler, slightly better                                   |
| **Three-Way Interaction**   | 0.9654      | 0.9695              | 0.830                | 565.02     | TV, Radio, Newspaper, TV×Radio, TV×Newspaper, Radio×Newspaper, TV×Radio×Newspaper | Strong synergy effects; good general fit                   |
| **Three-Way Quadratic**     | **0.9845**  | **0.6487**          | 0.587                | **407.15** | TV, Radio, Newspaper, All 2-way & 3-way interactions, TV², Radio², Newspaper²     | **Best overall performance**; highest R²                   |
| **Three-Way LASSO**         | N/A         | 1.624               | **0.6339**           | N/A        | Selected from all three-way + quadratic terms                                     | **Best predictive accuracy**; automatic variable selection |
| **Three-Way Random Forest** | N/A         | 1.490               | 0.676                | N/A        | All three-way + quadratic terms                                                   | Strong nonlinear modeling; good generalization             |

### Summary:

-   **Three-Way Quadratic** is the best **statistical model** — explains most variance, interpretable.

-   **Three-Way LASSO** is the best **predictive model** — best CV RMSE, automatic feature selection.

## Optimal Allocation from Interaction model

### Why the Interaction Model Is Best for Allocation?

The **Interaction Model** is the most appropriate choice for advertising budget allocation because it provides a **simple and interpretable formula** that still captures synergy between TV and Radio. Unlike the **Three-Way Quadratic Model**, which includes many higher-order and interaction terms (making symbolic optimization nearly impossible), the interaction model allows us to derive a **closed-form solution** using Lagrange multipliers.

Additionally, **Three-Way LASSO** is optimized for predictive accuracy — not interpretation — and shrinks some terms to zero, making it unreliable for marginal effect analysis. In contrast, the Interaction Model provides a **mathematically manageable** structure that supports meaningful economic insights and precise allocation strategies.

We aim to maximize the sales function:

$$
\text{Sales}(TV, Radio) = 6.586 + 0.01997 \cdot TV + 0.01928 \cdot Radio + 0.00115 \cdot TV \cdot Radio
$$

Subject to the constraint:

$$
TV + Radio = B
$$

We construct the Lagrangian function:

$$
\mathcal{L}(TV, Radio, \lambda) = 6.586 + 0.01997 \cdot TV + 0.01928 \cdot Radio + 0.00115 \cdot TV \cdot Radio - \lambda (TV + Radio - B)
$$

Take the partial derivatives:

$$
\frac{\partial \mathcal{L}}{\partial TV} = 0.01997 + 0.00115 \cdot Radio - \lambda = 0
$$

$$
\frac{\partial \mathcal{L}}{\partial Radio} = 0.01928 + 0.00115 \cdot TV - \lambda = 0
$$

$$
\frac{\partial \mathcal{L}}{\partial \lambda} = TV + Radio - B = 0
$$

Subtracting the first two equations to eliminate $\lambda$:

$$
(0.01997 + 0.00115 \cdot Radio) - (0.01928 + 0.00115 \cdot TV) = 0
$$

Simplifying:

$$
0.00115 \cdot (Radio - TV) = -0.00069
$$

So the optimal allocation condition is:

$$
\boxed{Radio = TV - 0.60}
$$

Substitute into the budget constraint $TV + Radio = B$:

$$
TV + (TV - 0.60) = B \Rightarrow 2TV = B + 0.60 \Rightarrow TV = \frac{B + 0.60}{2}
$$

$$
Radio = \frac{B - 0.60}{2}
$$

------------------------------------------------------------------------

## 

### Full Conclusion

In this study, we analyzed the Advertisement Sales dataset to investigate how advertising expenditures across TV, Radio, and Newspaper channels influence product sales. Multiple linear regression analysis initially confirmed that TV and Radio advertising budgets have statistically significant positive effects on sales, while Newspaper advertising was not a significant predictor. We then systematically compared a series of increasingly complex models—including full and reduced linear models, an interaction model, a three-way quadratic model, LASSO regression, and Random Forest—while checking model assumptions such as linearity, normality, constant variance, and absence of multicollinearity. No serious violations or outliers were found, supporting the reliability of our inferences.

Among all models, the **Three-Way Quadratic Model** demonstrated the best statistical performance, achieving the highest adjusted \$R\^2\$ of **0.9845** and the lowest residual standard error of **0.6487**, indicating it explained the greatest proportion of sales variation. In terms of predictive accuracy, **Three-Way LASSO** achieved the lowest cross-validated RMSE of **0.554**, making it the most effective for generalization. However, when it comes to **strategic resource allocation**, the **Interaction Model** was most appropriate due to its mathematical simplicity and interpretability. It captures the synergy between TV and Radio while enabling symbolic optimization via Lagrange multipliers.

From this model, we derived an updated optimal allocation rule:

$$
\boxed{Radio = TV - 0.60}
$$

This means that to maximize marginal sales gains, the Radio budget should be approximately **\$600 less** than the TV budget. Given a total advertising budget \$B\$, the optimal split is:

$$
TV = \frac{B + 0.60}{2}, Radio = \frac{B - 0.60}{2}
$$

These results offer meaningful guidance for marketing strategists. While complex models like three-way LASSO and quadratic regression provide high predictive power, the interaction model enables clear and actionable decisions for budget planning.

Future research could incorporate additional variables such as seasonality, competitive effects, or demographic segmentation to enhance predictive granularity. Causal inference techniques, including A/B testing or time-series modeling, would further validate advertising impact. Ultimately, this analysis highlights that coordinated investment in TV and Radio—guided by evidence-based allocation rules—can significantly improve advertising effectiveness and sales outcomes.

------------------------------------------------------------------------
